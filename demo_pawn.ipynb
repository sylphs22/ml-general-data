{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "IllegalArgumentException",
     "evalue": "'Delimiter cannot be more than one character: \\t+'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/home/bipul/Downloads/spark-2.0.2/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/bipul/Downloads/spark-2.0.2/python/lib/py4j-0.10.3-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    318\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o32.load.\n: java.lang.IllegalArgumentException: Delimiter cannot be more than one character: \t+\n\tat org.apache.spark.sql.execution.datasources.csv.CSVTypeCast$.toChar(CSVInferSchema.scala:313)\n\tat org.apache.spark.sql.execution.datasources.csv.CSVOptions.<init>(CSVOptions.scala:68)\n\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.inferSchema(CSVFileFormat.scala:54)\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$15.apply(DataSource.scala:421)\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$15.apply(DataSource.scala:421)\n\tat scala.Option.orElse(Option.scala:289)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:420)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:149)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:132)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:745)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-cc72495d1ab9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m                       \u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'com.databricks.spark.csv'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                       \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'true'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\t+'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'PERMISSIVE'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m                      inferSchema='true')\n\u001b[0m",
      "\u001b[0;32m/home/bipul/Downloads/spark-2.0.2/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbasestring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/bipul/Downloads/spark-2.0.2/python/lib/py4j-0.10.3-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/bipul/Downloads/spark-2.0.2/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     77\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mQueryExecutionException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'java.lang.IllegalArgumentException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mIllegalArgumentException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m             \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m: 'Delimiter cannot be more than one character: \\t+'"
     ]
    }
   ],
   "source": [
    "df = sqlContext.read.load(\"pawn.txt\", \n",
    "                      format='com.databricks.spark.csv', \n",
    "                      header='true', delimiter='\\t',mode='PERMISSIVE',\n",
    "                     inferSchema='true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pawn=df.rdd.map(lambda x:x.Description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pawn_all=df.rdd.map(lambda x:x).zipWithIndex().toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pawn_doc=df.rdd.map(lambda x:x.Description).zipWithIndex().toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---+\n",
      "|                  _1| _2|\n",
      "+--------------------+---+\n",
      "|1 Bracelet   yell...|  0|\n",
      "|1 Earring  14 kt ...|  1|\n",
      "|1 Earring  14 kt ...|  2|\n",
      "|1 Chain   yellow ...|  3|\n",
      "|1 Charm   yellow ...|  4|\n",
      "|14 kt yellow gold...|  5|\n",
      "|           Demo     |  6|\n",
      "+--------------------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pawn_doc.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "#from stemming.porter2 import stem\n",
    "def clean_word(w):\n",
    "    w=w.lower().strip()\n",
    "    w=re.sub('\\n',\" \",w)\n",
    "    #w=re.sub(r'\\s+', ' ', w).strip()\n",
    "    w=re.sub(\"[^a-z| |0-9]|,\\,|\\.|\\;|\\:|\\;|\\?|\\!|\\[|\\]|\\}|\\{(?i)\\b|[0-9]|((?:https?://|www\\d{0,3}))|[//]|[#]|[$]|\", \"\", (w.lower()))\n",
    "    w=re.sub('\\s\\s+', ' ', w)\n",
    "    return w.lower().strip()\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text=pawn_doc.rdd.map(lambda x: (clean_word(x[0]),x[1])).toDF().withColumnRenamed(\"_1\",\"doc_text\").withColumnRenamed(\"_2\",\"doc_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = (text\n",
    "      .rdd\n",
    "  .map(lambda x : (x.doc_id, x.doc_text.split(\" \")))\n",
    "  .toDF()\n",
    "  .withColumnRenamed(\"_1\",\"doc_id\")\n",
    "  .withColumnRenamed(\"_2\",\"features\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#df.withcolum(\"feature\",filter())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#kuchAur.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StopWordsRemover\n",
    "remover = StopWordsRemover(inputCol=\"features\", outputCol=\"tags\")\n",
    "df0=remover.transform(df)\n",
    "from pyspark.ml.feature import HashingTF\n",
    "from pyspark.ml.feature import IDF\n",
    "from pyspark.sql.types import DoubleType\n",
    "htf = HashingTF(inputCol=\"tags\", outputCol=\"tf\")\n",
    "tf = htf.transform(df0)\n",
    "tf1=tf.select(\"doc_id\",\"tags\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pawn_rest=pawn_all.rdd.map(lambda x: (x[0],x[1])).toDF().withColumnRenamed(\"_1\",\"Pawn\").withColumnRenamed(\"_2\",\"doc_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"Pawn\":{\"StoreName\":\"Hope Diamonds\",\"StoreAddress\":\"275 route 4 West, #12\",\"StoreCity\":\"Paramus\",\"StoreState\":\"NJ\",\"StoreZip\":7652,\"StorePhone\":\"(201)489-8898\",\"TransDate\":\"28/02/2017 18:02\",\"Amount\":259,\"LastName\":\"kartanos\",\"FirstName\":\"steve\",\"MiddleName\":\"NULL\",\"DBO\":\"08/04/1982 0:00\",\"Gender\":\"NULL\",\"Address\":\"384 LACEY DR\",\"City\":\"NEW MILFORD\",\"State\":\"NJ\",\"ZIP\":7646,\"HomePhone\":\"NULL\",\"WorkPhone\":\"NULL\",\"CellPhone\":\"NULL\",\"Description\":\"1 Bracelet   yellow  5.2 grams\",\"Make\":\"NULL\",\"Mode\":\"NULL\",\"Color\":\"yellow\",\"Serial\":\"NULL\",\"ItemAmount\":98},\"tags\":[\"bracelet\",\"yellow\",\"grams\"]}\n"
     ]
    }
   ],
   "source": [
    "in_df=pawn_rest.join(tf1,[\"doc_id\"]).drop(\"doc_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "in_df.coalesce(1).write.format('json').save('pawn.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'tags'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-f0415e8e99ba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoJSON\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfirst\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'tags'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = (text\n",
    "      .rdd\n",
    "  .map(lambda x : (x.doc_id,x.doc_text.split(\" \")))\n",
    "  .toDF()\n",
    "  .withColumnRenamed(\"_1\",\"doc_id\")\n",
    "  .withColumnRenamed(\"_2\",\"features\"))\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "remover = StopWordsRemover(inputCol=\"features\", outputCol=\"tags\")\n",
    "df0=remover.transform(df)\n",
    "from pyspark.ml.feature import HashingTF\n",
    "from pyspark.ml.feature import IDF\n",
    "from pyspark.sql.types import DoubleType\n",
    "htf = HashingTF(inputCol=\"tags\", outputCol=\"tf\")\n",
    "tf = htf.transform(df0)\n",
    "tf1=tf.select(\"doc_id\",\"tags\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "import numpy.linalg as LA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "#from stemming.porter2 import stem\n",
    "def clean_word(w):\n",
    "   # w=w.lower().strip()\n",
    "    w=re.sub('\\n',\" \",w)\n",
    "    return re.sub(\"[^a-z| |0-9]|,\\,|\\.|\\;|\\:|\\;|\\?|\\!|\\[|\\]|\\}|\\{(?i)\\b|[0-9]|((?:https?://|www\\d{0,3}))|[//]|[#]|[$]|\", \"\", (w.lower()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pawn_clean=pawn.map(lambda x: clean_word(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pawn1= pawn_clean.flatMap(lambda x: x.split(\",\")).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#def tfidf (text):\n",
    "vect = CountVectorizer(ngram_range=(1,2),max_df=0.7 ) #stop_words='None'\n",
    "vocabulary =  vect.fit(pawn1).vocabulary_\n",
    "tfidf_vect = TfidfVectorizer(vocabulary=vocabulary,ngram_range=(1,2),max_df=0.7) #stop_words='None'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feat_pawn=tfidf_vect.fit_transform(pawn1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType,LongType\n",
    "#schema = StructType([StructField(\"k\", StringType(), True), StructField(\"v\", IntegerType(), False)])\n",
    "#schema = StructType([StructField(\"postId\", LongType(), True),StructField(\"tags\", StringType(), True)])\n",
    "schema = StructType([StructField(\"tags\", StringType(), True)])\n",
    "#schema = StructType((\"tags\", StringType(), True))\n",
    "#schema = StructType([])\n",
    "empty_idn = sqlContext.createDataFrame(sc.emptyRDD(), schema)\n",
    "schema1 = StructType([StructField(\"postId\", StringType(), True)])\n",
    "#schema1 = StructType([])\n",
    "empty_idn1 = sqlContext.createDataFrame(sc.emptyRDD(), schema1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range (3):\n",
    "    idn=pawn_doc.select(pawn_doc._2).filter (pawn_doc._2 == i)\n",
    "    #idn=pawn_doc.filter(lambda x: x[1]== i).map (lambda x:x[1]).zipWithIndex().toDF()\n",
    "    #idn1=idn.select(\"_1\")\n",
    "    #idn1=idn.rdd.zipWithIndex().toDF().withColumnRenamed('_1', 'tags').withColumnRenamed('_2', 'index')\n",
    "    feature_array = np.array(tfidf_vect.get_feature_names())\n",
    "    tfidf_sorting = (feat_pawn[i].nonzero()[1])\n",
    "    top_n = feature_array[tfidf_sorting]\n",
    "    top=sc.parallelize([top_n],1).map(lambda x: str(x)).zipWithIndex().toDF()\n",
    "    top1=top.select(\"_1\")\n",
    "    #rdd = (idn.rdd.zip(top)) \n",
    "    #daf=sqlContext.createDataFrame(top)\n",
    "    #daf=idn1.join(top,[\"index\"]).drop(\"index\")\n",
    "    empty_idn=empty_idn.union(top1)\n",
    "    empty_idn1=empty_idn1.union(idn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "empty_id=empty_idn.rdd.zipWithIndex().toDF().withColumnRenamed('_1', 'tags').withColumnRenamed('_2', 'index')\n",
    "empty_id1=empty_idn1.rdd.zipWithIndex().toDF().withColumnRenamed('_1', 'postId').withColumnRenamed('_2', 'index')#f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "in_df=empty_id1.join(empty_id,[\"index\"]).drop(\"index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "indf1=in_df.rdd.map(lambda l: (l[0],l[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "indf1.coalesce(1).write.format('json').save('test1.json') ###write\n",
    "indf1.coalesce(1).format('json').save('test1.json') ###write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "multiline_rdd=sc.wholeTextFiles('SMALL.JSON')\n",
    "json_rdd = multiline_rdd.map(lambda x : x[1])\n",
    "from pyspark.sql import SQLContext\n",
    "#sqlContext = SQLContext(sc)\n",
    "#sqlContext=SQLContext(sc)\n",
    "d1=sqlContext.read.json(json_rdd)\n",
    "d3=d1.dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print (d3.printSchema())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "coll=d3.rdd.map(lambda x: (x.description,x.loc.coordinates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "collab=d3.rdd.map(lambda x:x.description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "collab_doc=d3.rdd.map(lambda x:(x.description,x.loc.coordinates)).zipWithIndex()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "collab_doc.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "collab_clean=collab.map(lambda x: clean_word(x))\n",
    "collab1= collab_clean.flatMap(lambda x: x.split(\",\")).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#def tfidf (text):\n",
    "vect = CountVectorizer(stop_words='english')\n",
    "vocabulary =  vect.fit(collab1).vocabulary_\n",
    "tfidf_vect = TfidfVectorizer(stop_words='english',vocabulary=vocabulary)\n",
    "    #return tfidf_vect.fit_transform(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feat=vect.transform(collab1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feature_names = vect.get_feature_names()\n",
    "list1=[]\n",
    "for i in range(len(collab1)):\n",
    "    for col in feat[i].nonzero()[1]:\n",
    "        if feature_names[col]== 'burglary':\n",
    "            list1.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "collab2=collab_doc.filter(lambda x: x[1]==x[1] in list1).map (lambda x: (x[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "collab2.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df=collab2.toDF().withColumnRenamed(\"_1\",\"doc_text\").withColumnRenamed(\"_2\",\"coord\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "zipc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def clean_zip(text):\n",
    "    text= str (text)\n",
    "    text=text.replace('<Zip:'  ,\"\")\n",
    "    text=text.replace('>',\"\")\n",
    "    text=text.replace('[',\"\")\n",
    "    text=text.replace (']',\"\")\n",
    "    text= [x.strip() for x in text.split(',')]\n",
    "    return [int(float(a)) for a in text]\n",
    "    #return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clean_zip(zipc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "zipc7=[int(float(a)) for a in zipc6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import split, explode\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import *\n",
    "import re\n",
    "def clean_coord(w):\n",
    "    w= str(w)\n",
    "    #w=re.sub('[\\\\]',\"\",w)\n",
    "    #w=w[:10]\n",
    "    w=str(w).replace('[','').replace(']','')\n",
    "    return w\n",
    "    #\n",
    "udf_clean_coord = udf(clean_coord, StringType())\n",
    "df1=df.withColumn(\"coordinates\", udf_clean_coord(\"coord\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'CMUTweetTagger'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-3aa10c98ab85>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mCMUTweetTagger\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m: No module named 'CMUTweetTagger'"
     ]
    }
   ],
   "source": [
    "import CMUTweetTagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "split_col = split(df1['coordinates'], ',')\n",
    "df1 = df1.withColumn('latitude',  (split_col.getItem(0)))#.drop(\"coord\")\n",
    "df2= df1.withColumn('longitude', (split_col.getItem(1))) #.drop (\"coord\")\n",
    "df2=df2.withColumn ('lat',df2.latitude.cast('float'))\n",
    "df2=df2.withColumn ('long',df2.longitude.cast('float'))\n",
    "#rawdata.price.cast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def zipcode1(x, y, z=5):\n",
    "    zipc=(zipcode.isinradius((x,y),z))\n",
    "    return zipc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "#udf_zipcode = udf(zipcode1,ArrayType(StringType()))\n",
    "udf_zipcode = udf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd_zipcode=df2.rdd.map(lambda x: zipcode1(x.long,x.lat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd_zipcode.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#zipc=zipcode.isinradius(cbus, 20)\n",
    "df3=df2.withColumn ('zipcode',udf_zipcode(df2.long,df2.lat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df2.select('zipcode').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "collab_burg1=collab2.zipWithIndex()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "collab_clean1=collab2.map(lambda x: clean_word(x))\n",
    "collab3= collab_clean1.flatMap(lambda x: x.split(\",\")).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#def tfidf (text):\n",
    "vect1 = CountVectorizer(stop_words='english')\n",
    "vocabulary1 =  vect1.fit(pawn1+collab3).vocabulary_\n",
    "tfidf_vect1 = TfidfVectorizer(stop_words='english',vocabulary=vocabulary)\n",
    "    #return tfidf_vect.fit_transform(text)\n",
    "                             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pawn_mat=tfidf_vect1.fit_transform(pawn1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "collab1_mat=tfidf_vect1.fit_transform(collab3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def broadcast_matrix(mat):\n",
    "    bcast = sc.broadcast((mat.data, mat.indices, mat.indptr))\n",
    "    (data, indices, indptr) = bcast.value\n",
    "    bcast_mat = csr_matrix((data, indices, indptr), shape=mat.shape)\n",
    "    return bcast_mat\n",
    "\n",
    "def parallelize_matrix(scipy_mat, rows_per_chunk=10):\n",
    "    [rows, cols] = scipy_mat.shape\n",
    "    i = 0\n",
    "    submatrices = []\n",
    "    while i < rows:\n",
    "        current_chunk_size = min(rows_per_chunk, rows - i)\n",
    "        submat = scipy_mat[i:i + current_chunk_size]\n",
    "        submatrices.append((i, (submat.data, submat.indices, \n",
    "                                submat.indptr),\n",
    "                            (current_chunk_size, cols)))\n",
    "        i += current_chunk_size\n",
    "    return sc.parallelize(submatrices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.sparse import csr_matrix\n",
    "a_mat_para = parallelize_matrix(collab1_mat, rows_per_chunk=100)\n",
    "b_mat_dist = broadcast_matrix(pawn_mat)\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def find_matches_in_submatrix(sources, targets, inputs_start_index,\n",
    "                              threshold=.1):\n",
    "    #count = 0\n",
    "    cosimilarities = cosine_similarity(sources, targets)\n",
    "    for i, cosimilarity in enumerate(cosimilarities):\n",
    "        cosimilarity = cosimilarity.flatten()\n",
    "        index=np.argsort(-cosimilarity).tolist()\n",
    "        target_index = index#\n",
    "        #target_index = cosimilarity.argsort()[-1]\n",
    "        #target_index = -cosimilarity.argsort().tolist()\n",
    "        source_index = inputs_start_index + i\n",
    "        for j in range(3):\n",
    "        #for target_index in enumerate(target_index):\n",
    "            target_index =index[j]\n",
    "            similarity = cosimilarity[target_index]\n",
    "            if cosimilarity[target_index] > threshold:\n",
    "                #k=k+j\n",
    "                #count += 1\n",
    "                yield (source_index, target_index,similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x= (a_mat_para.flatMap(lambda submatrix:find_matches_in_submatrix(csr_matrix(submatrix[1],shape=submatrix[2]),b_mat_dist,submatrix[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x.take(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y1=x.map (lambda x: x[0]).collect()\n",
    "y2=x.map (lambda x: x[1]).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType,StructField,IntegerType,StringType\n",
    "y=x.map(lambda x: (x[0],x[1],x[2]))\n",
    "#y1=(y.zipWithIndex().toDF().withColumnRenamed(\"_1\",\"DOC\").withColumnRenamed(\"_2\",\"Related_doc\").withColumnRenamed(\"_3\",\"id\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(6):##### change here\n",
    "    df4=collab_burg.filter(lambda x: x[1]==y1[i]).map (lambda x: x[0]).collect()\n",
    "    print (df4)\n",
    "    print ('='*60)\n",
    "               \n",
    "    df5=pawn_doc.filter(lambda x: x[1]==(y2[i])).map (lambda x: x[0]).collect()\n",
    "        \n",
    "    print (df5)\n",
    "    print ('-'*100 )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feature_names = tfidf_vect1.get_feature_names()\n",
    "for i in range (5): \n",
    "    a=y2[i]\n",
    "    b=y1[i]\n",
    "    for col in pawn_mat[a].nonzero()[1]:\n",
    "        for col1 in collab1_mat[b].nonzero()[1]:\n",
    "            if (pawn_mat[a,col]==collab1_mat[b,col1]):\n",
    "                print (feature_names[col], ' - ', pawn_mat[a, col])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for col in pawn_mat[0].nonzero()[1]:\n",
    "        for col1 in collab1_mat[0].nonzero()[1]:\n",
    "            if (pawn_mat[0,col]==collab1_mat[0,col1]):\n",
    "                print (feature_names[col], ' - ', pawn_mat[0, col])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print (y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
