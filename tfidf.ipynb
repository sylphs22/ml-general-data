{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.context.SparkContext at 0x7f3ffdc26470>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc##################can we have synonyms in tfidf matrix??????"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sqlContext = SQLContext(sc)\n",
    "#df=sc.textFile('SMALL.JSON')\n",
    "df = sqlContext.read.json('SMALL.JSON')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df1=df.select(\"description\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df2=df1.dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- description: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc=df2.rdd.zipWithIndex()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df3=doc.toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _1: struct (nullable = true)\n",
      " |    |-- description: string (nullable = true)\n",
      " |-- _2: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "#from stemming.porter2 import stem\n",
    "def clean_word(w):\n",
    "    w= str(w)\n",
    "    w=re.sub('\\n',\"\",w)\n",
    "    return re.sub(\"[^a-z| |0-9]|,\\,|\\.|\\;|\\:|\\;|\\?|\\!|\\[|\\]|\\}|\\{(?i)\\b|[0-9]|((?:https?://|www\\d{0,3}))|[//]|[#]|[$]|\", \"\", (w.lower()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text=df3.rdd.map(lambda x: (clean_word(x[0]),x[1])).toDF().withColumnRenamed(\"_1\",\"doc_text\").withColumnRenamed(\"_2\",\"doc_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Can not infer schema for type: <class 'str'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-31aa0530c54f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mclean_word\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoDF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumnRenamed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"_1\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"doc_text\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/bipul/Downloads/spark-2.0.2/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36mtoDF\u001b[0;34m(self, schema, sampleRatio)\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'Alice'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \"\"\"\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampleRatio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0mRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoDF\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtoDF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/bipul/Downloads/spark-2.0.2/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36mcreateDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    520\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRDD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 522\u001b[0;31m             \u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_createFromRDD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    523\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m             \u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_createFromLocal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/bipul/Downloads/spark-2.0.2/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36m_createFromRDD\u001b[0;34m(self, rdd, schema, samplingRatio)\u001b[0m\n\u001b[1;32m    358\u001b[0m         \"\"\"\n\u001b[1;32m    359\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 360\u001b[0;31m             \u001b[0mstruct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inferSchema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    361\u001b[0m             \u001b[0mconverter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_create_converter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstruct\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m             \u001b[0mrdd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/bipul/Downloads/spark-2.0.2/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36m_inferSchema\u001b[0;34m(self, rdd, samplingRatio)\u001b[0m\n\u001b[1;32m    338\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msamplingRatio\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 340\u001b[0;31m             \u001b[0mschema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_infer_schema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfirst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    341\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0m_has_nulltype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/bipul/Downloads/spark-2.0.2/python/pyspark/sql/types.py\u001b[0m in \u001b[0;36m_infer_schema\u001b[0;34m(row)\u001b[0m\n\u001b[1;32m    990\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    991\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 992\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Can not infer schema for type: %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    993\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    994\u001b[0m     \u001b[0mfields\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mStructField\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_infer_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Can not infer schema for type: <class 'str'>"
     ]
    }
   ],
   "source": [
    "text=df2.rdd.map(lambda x: (clean_word(x))).toDF().withColumnRenamed(\"_1\",\"doc_text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = (text\n",
    "      .rdd\n",
    "  .map(lambda x : (x.doc_id,x.doc_text.split(\" \")))\n",
    "  .toDF()\n",
    "  .withColumnRenamed(\"_1\",\"doc_id\")\n",
    "  .withColumnRenamed(\"_2\",\"features\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- doc_id: long (nullable = true)\n",
      " |-- features: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StopWordsRemover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "remover = StopWordsRemover(inputCol=\"features\", outputCol=\"filtered\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df1=remover.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF\n",
    "from pyspark.ml.feature import IDF\n",
    "from pyspark.sql.types import DoubleType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "htf = HashingTF(inputCol=\"filtered\", outputCol=\"tf\",numFeatures=1000)\n",
    "tf = htf.transform(df1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.cache()\n",
    "idf = IDF(inputCol=\"tf\", outputCol=\"tfidf\")\n",
    "tfidf = idf.fit(tf).transform(tf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mat1=tfidf.rdd.map(lambda x: (x.tfidf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "def as_matrix(vec):\n",
    "    data, indices = vec.values, vec.indices\n",
    "    shape = 1, vec.size\n",
    "    return csr_matrix((data, indices, np.array([0, vec.values.size])), shape)\n",
    "\n",
    "mats = mat1.map(as_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.sparse import vstack\n",
    "\n",
    "mat1 = mats.reduce(lambda x, y: vstack([x, y]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mat1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cosimilarities = cosine_similarity(mat1, mat1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "index=np.argsort(-cosimilarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Document Similarity Analysis using Cosine Similarity\n",
      "\n",
      "============================================================\n",
      "documents: 1 : [Row(description='Date: 12/10/2010 Case #: 10-12574 Author ID: SRK-241\\n\\nOn December 2, 2010 the above male party entered Allendale Liquors to cash a check. The suspect\\nhanded over a FED-EX check with a Wachovia Bank label. The suspect produced a NJ Temporary\\nDrivers License which had been altered. The suspect also showed the employee what appeared to\\nbe a FED-EX employee identification card but did not hand it over.\\n\\nInfo on the Temp NJ DL: Lixander Viveros, 33 Lake St. Mahwah NJ 07446 is a bad address.\\n\\nAny agency that has similar or could ID suspect is asked to contact P.O. Scott Kuenzel or Det. John\\nMattiace.\\nAllendale Police Dept. - Headquarters\\n(201) 825-1900\\n')]\n",
      "----------------------------------------------------------------------------------------------------\n",
      " similar documents\n",
      " Top Doc: 1 :\n",
      " [Row(description='Date: 5/23/2011 Case #: 11-18434 Author ID: 516\\nCraig Avent, DOB/9-25-1976, Rahway N.J. 2003 GMC Yukon, NJ VKR83V (courtesy PlateScan)\\nOn May 20, 2011 at 10:07 PM the above individual and vehicle were observed in the parking lot of a\\ntrucking company located at 340 S. Stiles St. The suspect appeared to be looking at trailers parked\\nin the lot. There have been several burglaries and thefts of trailers at this location. When the suspect\\nwas confronted by an employee, he stated he was looking for a place to go to the bathroom. When\\nthe employee told the suspect he was calling the police, the suspect also called the police, eagerly\\nwanting to give his side of the story. The suspect appeared nervous while being questioned. The\\nemployee told officers he recognized the suspect as a truck driver himself whom he has seen in an\\nElizabeth diner on occasions. At this time , the suspect has not been charged with any crime. He\\nwas allowed to proceed on his way.\\n\\nAnyone with information on this suspect is asked to contact Det. Frank Leporino at 908-474-8535.\\nLinden Police Dept. - Detective Bureau\\n(908) 474-8537\\n')]\n",
      "----------------------------------------------------------------------------------------------------\n",
      " similar documents\n",
      " Top Doc: 2 :\n",
      " [Row(description=\"The Wyckoff Police attempting to identify the above female who tried to cash a $4,000. check at our Chase Bank on 5/13/14. The suspect had a fraudulent NY DL in the victims name with suspect's picture on it. The victim does reside in NY.\\r\\n\\r\\nDuring this mid-day transaction attempt, the bank became suspicious and asked the suspect for a second form of ID. The suspect stated that she had it in her car, walked outside, and fled the scene (no further information). \\r\\n\\r\\nOn 5/20/14, a Wyckoff resident (male) also reported that Chase Bank contacted him and stated that someone with a NY DL in his name attempted to cash a check at a Chase Branch in West Caldwell, NJ. The male suspect also had a fraudulent NY DL in his possession as ID.\\r\\n\\r\\nAny agencies with information or similar incidents can contact the Wyckoff Police Detective Bureau @ 201-891-2121\")]\n",
      "============================================================\n",
      "Document Similarity Analysis using Cosine Similarity\n",
      "\n",
      "============================================================\n",
      "documents: 2 : [Row(description='Date: 12/14/2011 Case #: 2011-542 Author ID: 255JB\\nNOT ACTUAL VEHICLE/ NJ ZRN-93C NOT ACTUAL VEHICLE/ NJ ZRN-93C\\n******* *************** CANCEL.. VEHICLE LOCATED IN VAILSBURG..***************************\\n\\nTHANKS TO ALL DEPARTMENTS FOR THIER ASSISTANCE.\\nHAVE A SAFE AND HAPPY HOLIDAY..\\n\\n\\n\\n\\n\\nStolen from driveway of residence between 2100hrs-12/12 & 0700-12/13.\\n2007 Mercedes Benz GL450, NJ REG: ZRN-93C, white in color, tan interior. Vehicle was unlocked,\\nkeys in center cup holder. Both front and back license plates have a \"ARBONNE COSMETICS\"\\nframe around them.\\nOwner has valid registration and insurance in her possession, expired ones MAY be in vehicle.\\nRear storage area had several boxes of used books as well as a Kohls bag of used clothing. Back\\nseat had a Nikon D5000 SLR camera with 2 lens in case.\\n\\nAnyone locating this vehicle please contact this agency and secure for processing.\\nShip Bottom NJ Police -\\n(609) 494-3055/ 494-1518\\n')]\n",
      "----------------------------------------------------------------------------------------------------\n",
      " similar documents\n",
      " Top Doc: 1 :\n",
      " [Row(description=\"On 07/08/2014 the victim reported her vehicle missing at approximately 1030 hours.  The caller stated the vehicle is paid off and should not have been repossessed.  Officers observed tire markings in the victim's dirt driveway indicating that the vehicle was towed from the driveway.  The keys are not with the vehicle.\\r\\n\\r\\nThe vehicle is described as a 2007, Blue, KIA Optima bearing New Jersey registration V84EKJ.\\r\\n\\r\\nIf the vehicle is recovered please contact the Gloucester Township Police Department at 856-228-4500.\\r\\n\\r\\n**The above pictured vehicle is not the actual vehicle.**\")]\n",
      "----------------------------------------------------------------------------------------------------\n",
      " similar documents\n",
      " Top Doc: 2 :\n",
      " [Row(description='The Green Brook Police Department is Investigating a motor vehicle theft that occurred between the hours of 1830 and 2200, on Friday night 05/16/2014 from the driveway of 2 Kelly Court, Green Brook, NJ 08812. The Vehicle is a 2005 Porsche Cayenne color White, parked in the driveway of the residence. The vehicle has a \"\"Fairfield University\"\" sticker on the rear window. The vehicle also has \"\"Shore to Please\"\" Specialty New Jersey License plates bearing registration SN185X. The vehicle keys may be accompanied with the vehicle. Vehicle VIN# WP1AA29P95LA24931.\\r\\n\\r\\nAny Information regarding this vehicle or similar circumstances should be relayed to:\\r\\nPolice Officer Daniel R. Dix #314, Green Brook Police Department. (732) 968-1188')]\n",
      "============================================================\n",
      "Document Similarity Analysis using Cosine Similarity\n",
      "\n",
      "============================================================\n",
      "documents: 3 : [Row(description='\\nEvent Description:  The NJ ROIC is advising that Trenton P.D. has confirmed a shooting at the above location.  The male victim sustained a gun shot wound to the lower back and was transported to St. Francis Medical Center by personal vehicle.\\n\\nNotification: N/A\\n\\nResponse:  N/A\\n\\nDistributed to: SHG\\nTD/PK\\n\\n')]\n",
      "----------------------------------------------------------------------------------------------------\n",
      " similar documents\n",
      " Top Doc: 1 :\n",
      " [Row(description='\\nEvent Description: The NJ ROIC is advising that Paterson P.D. has confirmed a shooting at the above location.  A male victim sustained a non-life threatening gun shot wound to the right leg.\\n\\nNotification: N/A\\n\\nResponse: N/A\\n\\nDistribution: SHG\\nTD/MB\\n\\n')]\n",
      "----------------------------------------------------------------------------------------------------\n",
      " similar documents\n",
      " Top Doc: 2 :\n",
      " [Row(description='\\nEvent Description: The NJ ROIC is advising that Newark P.D. has confirmed a shooting at the above location.  The victim sustained non-life threatening gun shot wounds to the head, neck and hip.  He was transported to UMDNJ by personal vehicle.\\n\\nNotification: N/A\\n\\nResponse: N/A\\n\\nDistribution: SHG\\nTD/MB\\n\\n')]\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):##### change here\n",
    "    a=(index[i]).tolist()\n",
    "    df4=doc.filter(lambda x: x[1]==a[0]).map (lambda x: x[0])\n",
    "    print ('='*60)\n",
    "    print ('Document Similarity Analysis using Cosine Similarity\\n')\n",
    "    print ('='*60)\n",
    "    print (\"documents:\",i+1,\":\",df4.collect())\n",
    "   # print (df3.collect())    \n",
    "    for j in (range(2)):\n",
    "        df5=doc.filter(lambda x: x[1]==a[j+1]).map (lambda x: x[0])\n",
    "        print ('-'*100 )\n",
    "        print (\" similar documents\\n\",\"Top Doc:\", j+1,\":\\n\",df5.collect() )\n",
    "       # print (df2.collect())\n",
    "        print ('-'*100 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tags=text.map(lambda line: line.split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.feature import HashingTF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hashingTF = HashingTF(1000)\n",
    "tf = hashingTF.transform(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.feature import IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.cache()\n",
    "idf = IDF().fit(tf)\n",
    "tfidf = idf.transform(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import SparseVector\n",
    "from operator import attrgetter\n",
    "\n",
    "df = mats.toDF([\"features\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg.distributed import RowMatrix,IndexedRowMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mat=RowMatrix (tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rows = mat.rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "def as_matrix(vec):\n",
    "    data, indices = vec.values, vec.indices\n",
    "    shape = 1, vec.size\n",
    "    return csr_matrix((data, indices, np.array([0, vec.values.size])), shape)\n",
    "\n",
    "mats = rows.map(as_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mats.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.sparse import vstack\n",
    "\n",
    "mat1 = mats.reduce(lambda x, y: vstack([x, y]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cosimilarities = cosine_similarity(mat1, mat1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cosimilarities.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
