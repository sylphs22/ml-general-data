{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##conf.setMaster(\"yarn-client\").setAppName(\"My app\")\n",
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)\n",
    "import json\n",
    "multiline_rdd=sc.wholeTextFiles('train.JSON')\n",
    "json_rdd = multiline_rdd.map(lambda x : x[1])\n",
    "d=sqlContext.read.json(json_rdd)\n",
    "#d = sqlContext.read.json('train.JSON')\n",
    "df1=d.select(\"description\")\n",
    "df2=df1.dropDuplicates()\n",
    "doc=df2.rdd.zipWithIndex()\n",
    "df3=doc.toDF()\n",
    "import re\n",
    "#from stemming.porter2 import stem\n",
    "def clean_word(w):\n",
    "    w= str(w).replace('description','').replace('Notification','').replace('Response','').replace('Distribution','').replace('N/A','')\n",
    "    w=str (w).replace('Row','').replace ('\\n\\n','')\n",
    "    w=re.sub(r'\\n+',\"\",w)\n",
    "    w=re.sub(r'\\\\n+',\"\",w)\n",
    "    return re.sub(r\"[^a-z| |0-9]|,\\,|\\.|\\;|\\:|\\;|\\?|\\!|\\[|\\]|\\}|\\{(?i)\\b|[0-9]|((?:https?://|www\\d{0,3}))|[//]|[#]|[$]|\", \"\", (w.lower()))\n",
    "\n",
    "text=df3.rdd.map(lambda x: (clean_word(x[0]),x[1])).toDF().withColumnRenamed(\"_1\",\"doc_text\").withColumnRenamed(\"_2\",\"doc_id\")\n",
    "df = (text\n",
    "      .rdd\n",
    "  .map(lambda x : (x.doc_id,x.doc_text))  #.map(lambda x : (x.doc_id,x.doc_text.split(\" \")))\n",
    "  .toDF()\n",
    "  .withColumnRenamed(\"_1\",\"doc_id\")\n",
    "  .withColumnRenamed(\"_2\",\"features\"))\n",
    "from pyspark.ml.feature import HashingTF,Tokenizer\n",
    "from pyspark.ml.feature import IDF\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"features\", outputCol=\"words\")\n",
    "df4 = tokenizer.transform(df)\n",
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\")\n",
    "df5=remover.transform(df4)\n",
    "\n",
    "htf = HashingTF(inputCol=\"filtered\", outputCol=\"tf\",numFeatures=100)##################chnage the number of features#########\n",
    "tf = htf.transform(df5)\n",
    "tf.cache()\n",
    "idf = IDF(inputCol=\"tf\", outputCol=\"tfidf\")\n",
    "tfidf = idf.fit(tf).transform(tf)\n",
    "#tfidf=tf\n",
    "mat1=tfidf.rdd.map(lambda x: (x.tfidf))\n",
    "#mat1=tfidf.rdd.map(lambda x: (x.tf))\n",
    "from scipy.sparse import csr_matrix\n",
    "def as_matrix(vec):\n",
    "    data, indices = vec.values, vec.indices\n",
    "    shape = 1, vec.size\n",
    "    return csr_matrix((data, indices, np.array([0, vec.values.size])), shape)\n",
    "\n",
    "mats = mat1.map(as_matrix)\n",
    "import numpy as np\n",
    "from scipy.sparse import vstack\n",
    "\n",
    "mat2 = mats.reduce(lambda x, y: vstack([x, y]))\n",
    "def broadcast_matrix(mat):\n",
    "    bcast = sc.broadcast((mat.data, mat.indices, mat.indptr))\n",
    "    (data, indices, indptr) = bcast.value\n",
    "    bcast_mat = csr_matrix((data, indices, indptr), shape=mat.shape)\n",
    "    return bcast_mat\n",
    "\n",
    "def parallelize_matrix(scipy_mat, rows_per_chunk=100):\n",
    "    [rows, cols] = scipy_mat.shape\n",
    "    i = 0\n",
    "    submatrices = []\n",
    "    while i < rows:\n",
    "        current_chunk_size = min(rows_per_chunk, rows - i)\n",
    "        submat = scipy_mat[i:i + current_chunk_size]\n",
    "        submatrices.append((i, (submat.data, submat.indices, \n",
    "                                submat.indptr),\n",
    "                            (current_chunk_size, cols)))\n",
    "        i += current_chunk_size\n",
    "    return sc.parallelize(submatrices)\n",
    "    \n",
    "mat3=mat2.tocsr()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf.select('filtered','tf').show(3,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf.select('tfidf').show (3,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df1.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_matches_in_submatrix(sources, targets, inputs_start_index,\n",
    "                              threshold=.0):\n",
    "    cosimilarities = cosine_similarity(sources, targets)\n",
    "    for i, cosimilarity in enumerate(cosimilarities):\n",
    "        cosimilarity = cosimilarity.flatten()\n",
    "        index=np.argsort(-cosimilarity).tolist()\n",
    "        target_index = index#\n",
    "        #target_index = cosimilarity.argsort()[-1]\n",
    "        #target_index = -cosimilarity.argsort().tolist()\n",
    "        source_index = inputs_start_index + i\n",
    "        for j in range(3):\n",
    "        #for target_index in enumerate(target_index):\n",
    "            target_index =index[j]\n",
    "            similarity = cosimilarity[target_index]\n",
    "        #if cosimilarity[target_index] >= threshold:\n",
    "            yield (source_index, target_index, similarity)\n",
    "            #yield (target_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "multiline_rdd=sc.wholeTextFiles('test.JSON')\n",
    "json_rdd = multiline_rdd.map(lambda x : x[1])\n",
    "d_test=sqlContext.read.json(json_rdd)\n",
    "\n",
    "#d_test = sqlContext.read.json('test.JSON')\n",
    "df1_test=d_test.select(\"description\")\n",
    "df2_test=df1_test.dropDuplicates()\n",
    "doc_test=df2_test.rdd.zipWithIndex()\n",
    "df3_test=doc_test.toDF()\n",
    "import re\n",
    "#from stemming.porter2 import stem\n",
    "def clean_word(w):\n",
    "    w= str(w).replace('description','').replace('Notification','').replace('Response','').replace('Distribution','').replace('N/A','')\n",
    "    w=str (w).replace('Row','').replace ('\\n\\n','')\n",
    "    w=re.sub(r'\\n+',\"\",w)\n",
    "    w=re.sub(r'\\\\n+',\"\",w)\n",
    "    return re.sub(r\"[^a-z| |0-9]|,\\,|\\.|\\;|\\:|\\;|\\?|\\!|\\[|\\]|\\}|\\{(?i)\\b|[0-9]|((?:https?://|www\\d{0,3}))|[//]|[#]|[$]|\", \"\", (w.lower()))\n",
    "\n",
    "text_test=df3_test.rdd.map(lambda x: (clean_word(x[0]),x[1])).toDF().withColumnRenamed(\"_1\",\"doc_text\").withColumnRenamed(\"_2\",\"doc_id\")\n",
    "df_test = (text_test\n",
    "      .rdd\n",
    "  .map(lambda x : (x.doc_id,x.doc_text))\n",
    "  .toDF()\n",
    "  .withColumnRenamed(\"_1\",\"doc_id\")\n",
    "  .withColumnRenamed(\"_2\",\"features\"))\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"features\", outputCol=\"words\")\n",
    "df4_test = tokenizer.transform(df_test)\n",
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\")\n",
    "df5_test=remover.transform(df4_test)\n",
    "#htf_test = HashingTF(inputCol=\"filtered\", outputCol=\"tf\",numFeatures=100)##################chnage the number of features#########\n",
    "tf_test = htf.transform(df5_test)\n",
    "#tf_test.cache()\n",
    "#idf_test = IDF(inputCol=\"tf\", outputCol=\"tfidf\")\n",
    "#tfidf_test = idf.fit(tf_test).transform(tf_test)\n",
    "#tfidf = idf.fit(tf).transform(tf)\n",
    "tfidf_test=tf_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf_test.select('filtered','tf').show(3,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf_test.select('tfidf').show(1,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trenton, pd, confirmed, shooting, listed, location, , male, sustained, gun, shot, wounds, legs, injuries, described, non, life, threatening, trenton, pd, seeking, identified, black, male, suspect, , , shgrz]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#mat1_test=tfidf_test.rdd.map(lambda x: (x.tfidf))\n",
    "mat1_test=tfidf_test.rdd.map(lambda x: (x.tf))\n",
    "mats_test = mat1_test.map(as_matrix)\n",
    "mat2_test = mats_test.reduce(lambda x, y: vstack([x, y]))\n",
    "mat3_test =mat2_test.tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a_mat_para = parallelize_matrix(mat3_test, rows_per_chunk=100)\n",
    "b_mat_dist = broadcast_matrix(mat3)\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x= (a_mat_para.flatMap(lambda submatrix:find_matches_in_submatrix(csr_matrix(submatrix[1],shape=submatrix[2]),b_mat_dist,submatrix[0])))\n",
    "y=x.map(lambda x: (x[0],x[1],x[2]))\n",
    "#from pyspark.sql.types import StructType,StructField,IntegerType,StringType\n",
    "#schema = StructType([StructField(\"DOC\",IntegerType(), True),StructField(\"Related_doc\", IntegerType(), True),StructField(\"Related_num\", StringType(), True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#for i in range(3):##### change here\n",
    "    #a=(y1[i])\n",
    "df4=text_test.filter(lambda x: x[0]==0).map (lambda x: str(x[1])).collect()\n",
    "y2=y.filter(lambda x: x[0]==0).map (lambda x: x[1])\n",
    "y4=y2.take(3)\n",
    "for j in (range(3)):\n",
    "        #k=y1.filter(y1.DOC==i)\n",
    "        \n",
    "    df5=text.filter(lambda x: x[0]==(y4[j])).map (lambda x: x[1]).collect()\n",
    "    print (str (df4),str (df5),str (j+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
