{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-0840600e5843>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstem\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblob\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTextBlob\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtextblob_aptagger\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPerceptronTagger\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named 'text'"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from operator import add\n",
    "#from itertools import tee, izip\n",
    "import numpy as np\n",
    "import nltk.data\n",
    "from nltk import stem\n",
    "from text.blob import TextBlob as tb\n",
    "from textblob_aptagger import PerceptronTagger\n",
    "\n",
    "TOKENIZER = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "TAGGER = PerceptronTagger()\n",
    "STEMMER = stem.porter.PorterStemmer()\n",
    "\n",
    "\n",
    "def pos_tag (s):\n",
    "  \"\"\"high-performance part-of-speech tagger\"\"\"\n",
    "  global TAGGER\n",
    "  return TAGGER.tag(s)\n",
    "\n",
    "\n",
    "def wrap_words (pair):\n",
    "  \"\"\"wrap each (word, tag) pair as an object with fully indexed metadata\"\"\"\n",
    "  global STEMMER\n",
    "  index = pair[0]\n",
    "  result = []\n",
    "  for word, tag in pair[1]:\n",
    "    word = word.lower()\n",
    "    stem = STEMMER.stem(word)\n",
    "    keep = tag in ('JJ', 'NN', 'NNS', 'NNP',)\n",
    "    result.append({ \"index\": index, \"stem\": stem, \"word\": word, \"tag\": tag, \"keep\": keep })\n",
    "    index += 1\n",
    "  return result\n",
    "\n",
    "\n",
    "def sliding_window (iterable, size):\n",
    "  \"\"\"apply a sliding window to produce 'size' tiles\"\"\"\n",
    "  iters = tee(iterable, size)\n",
    "  for i in xrange(1, size):\n",
    "    for each in iters[i:]:\n",
    "      next(each, None)\n",
    "  return list(izip(*iters))\n",
    "\n",
    "\n",
    "def keep_pair (pair):\n",
    "  \"\"\"filter the relevant linked word pairs\"\"\"\n",
    "  return pair[0][\"keep\"] and pair[1][\"keep\"] and (pair[0][\"word\"] != pair[1][\"word\"])\n",
    "\n",
    "\n",
    "def link_words (seq):\n",
    "  \"\"\"attempt to link words in a sentence\"\"\"\n",
    "  return [ (seq[0], word) for word in seq[1:] ]\n",
    "\n",
    "\n",
    "def compute_contribs (links, rank):\n",
    "  \"\"\"calculate link contributions to the rank of other links\"\"\"\n",
    "  num_links = len(links)\n",
    "  for link in links:\n",
    "    yield (link, rank / num_links,)\n",
    "\n",
    "\n",
    "\n",
    "def glean_rank (pair):\n",
    "  \"reorder the word metdata into (K,V) pairs for sorting\"\n",
    "  key = pair[0]\n",
    "  w = pair[1][0]\n",
    "  rank = pair[1][1]\n",
    "  return w[\"index\"], (w[\"word\"], rank,)\n",
    "\n",
    "\n",
    "def extract_phrases (doc):\n",
    "  \"\"\"extract the top-ranked keyphrases from the given tagged doc\"\"\"\n",
    "  last_index = -1000\n",
    "  last_rank = 0.0\n",
    "  result = []\n",
    "  for index, (word, rank) in doc:\n",
    "    if index - last_index > 1:\n",
    "      if len(result) > 0:\n",
    "        yield last_rank * len(result), ' '.join(result)\n",
    "      last_rank = 0.0\n",
    "      result = []\n",
    "    last_rank += rank\n",
    "    last_index = index\n",
    "    result.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TEXT = \"\"\"\n",
    "Compatibility of systems of linear constraints over the set of natural numbers. \n",
    "Criteria of compatibility of a system of linear Diophantine equations, strict\n",
    "inequations, and nonstrict inequations are considered. Upper bounds for\n",
    "components of a minimal set of solutions and algorithms of construction of\n",
    "minimal generating sets of solutions for all types of systems are given. \n",
    "These criteria and the corresponding algorithms for constructing a minimal\n",
    "supporting set of solutions can be used in solving all the considered types\n",
    "systems and systems of mixed types.\n",
    "\"\"\"\n",
    "\n",
    "# construct a fully tagged document that is segmented into sentences\n",
    "\n",
    "sent = sc.parallelize(TOKENIZER.tokenize(TEXT)).map(pos_tag).cache()\n",
    "base = list(np.cumsum(np.array(sent.map(len).collect())))\n",
    "base.insert(0, 0)\n",
    "base.pop()\n",
    "lens = sc.parallelize(base)\n",
    "\n",
    "tagged_doc = lens.zip(sent).map(wrap_words).cache()\n",
    "tagged_doc.collect()\n",
    "\n",
    "# apply a sliding window to construct a graph of the relevant word pairs\n",
    "\n",
    "tiled = tagged_doc.flatMap(lambda s: sliding_window(s, 3)).flatMap(link_words).filter(keep_pair)\n",
    "t0 = tiled.map(lambda link: (link[0][\"stem\"], link[1][\"stem\"],))\n",
    "t1 = tiled.map(lambda link: (link[1][\"stem\"], link[0][\"stem\"],))\n",
    "neighbors = t0.union(t1)\n",
    "\n",
    "# visualize in a graph\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "\n",
    "G = nx.Graph()\n",
    "\n",
    "for a, b in neighbors.collect():\n",
    "  G.add_edge(a, b, weight=1.0)\n",
    "\n",
    "edges = [ (u,v) for (u,v,d) in G.edges(data=True) ]\n",
    "\n",
    "pos = nx.spring_layout(G)\n",
    "nx.draw_networkx_nodes(G, pos, node_size=700)\n",
    "nx.draw_networkx_edges(G, pos, edgelist=edges, width=6)\n",
    "nx.draw_networkx_labels(G, pos, font_size=20, font_family='sans-serif')\n",
    "\n",
    "plt.axis('off')\n",
    "plt.savefig(\"weighted_graph.png\")\n",
    "plt.show()\n",
    "\n",
    "# run TextRank, then extract the top-ranked keyphrases\n",
    "\n",
    "rank = sc.parallelize(text_rank_graph(neighbors))\n",
    "tags = tagged_doc.flatMap(lambda x: x).map(lambda w: (w[\"stem\"], w,))\n",
    "l = tags.join(rank).map(glean_rank).sortByKey().collect()\n",
    "\n",
    "for rank, phrase in sorted(set(extract_phrases(l)), reverse=True):\n",
    "  print \"%0.2f %s\" % (rank, phrase,)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
